---
title: "Our findings"
author:
  - name: Matthias Felix
    email: matthias.felix@uzh.ch
    affiliation: 11-746-625
  - name: Florian Fischer
    email: florian.fischer2@uzh.ch
    affiliation: 11-611-985
  - name: Catrin Loch
    email: catrin.loch@uzh.ch
    affiliation: 13-718-143

bibliography: sigproc.bib
output: rticles::acm_article
---

```{r setup, include=FALSE}
import::from(lmerTest, lmer)
import::from(car, qqPlot, ncvTest, Anova)
import::from(MASS, stepAIC)
import::from(multcomp, glht, mcp)
import::from(broom, tidy)
import::from(modelr, add_residuals, gather_residuals)
import::from(ggpubr, theme_pubclean)
import::from(nortest, lillie.test)
import::from(lawstat, levene.test)
library(tidyverse)
```



```{r, echo=FALSE, fig.height=2, fig.cap = "Example figure caption. \\label{fig:fig_example}"}
tibble(x = rnorm(30), y = rnorm(30)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_pubclean(base_size = 18)
```


```{r include=FALSE}
#Format number
test_value <- 1.23456
format_num <- function(nums, sigdigits = 3) gsub("\\.$", "", formatC(nums, sigdigits, format = "fg", flag="#"))
```

## Data Analysis

```{r include=FALSE}
# matriculation_nos <- c(11746625, 11611985, 13718143)

# load the  whole dataset
# sim_data <- readRDS("../data/sim_data.RDS")

# set randomization seed
# group_seed <- mean(matriculation_nos) %% 28
# set.seed(group_seed)

# randomly sample some participants out of the given dataset
#my_data <- 
 # sim_data %>% 
  # filter(Participant %in% sample(as.character(unique(sim_data$Participant)), 15))

# To ensure that your dataset doesn't change, run the code above only once.
# Afterwards, save and load data with the following command
saveRDS(my_data, file = "../data/my_data.RDS")
data <- readRDS("../data/my_data.RDS")
```

# Visualize data
```{r echo=FALSE}
p_base <-
  data %>%
  ggplot(aes(x = Condition, y = TP_bps)) +
  geom_violin() +
  geom_point(color = "lightgray")

p_base
```

# Explore assupmtion for the subset of data
## Normality
```{r echo=FALSE}
par(mfrow = c(1,4)) #plot four plots next to each other
data %>%
  group_by(Condition) %>%
  do({
    qqPlot(.$TP_bps)
    title(main=.$Condition[[1]])
    tidy(lillie.test(.$TP_bps))
  })
par(mfrow = c(1,1)) # sets back the default

```

Interpretation: The normality assumption does not hold for the subset of the data. The null hypothesis is that the data is normally distributed. Since the p-value is very small (< 0.05), we can reject this null hypothesis and cannot assume that the data is normally distributed. An exception has to be made for the condition Phone sway which is normally distributed (p = 0.17).

## Homogeneity
```{r echo=FALSE}
levene.test(data$TP_bps, data$Condition, location = "median")
```
Interpretation: the null hypothesis is that the data have a constant variance. Since the p-value is very small, we can reject the hypothesis  and cannot assume that the variance is homogeneous.

# Transformation
We perform a logarithmic transformation on the data and test the normality and homogeneity assumption again.

## Normality
```{r echo=FALSE}
par(mfrow = c(1,4)) #plot four plots next to each other
data %>%
  group_by(Condition) %>%
  do({
    qqPlot(log(log(.$TP_bps + 2)))
    title(main=.$Condition[[1]])
    tidy(lillie.test(log(log(.$TP_bps + 2))))
  })
par(mfrow = c(1,1)) # sets back the default

```

## Homogeneity
```{r echo=FALSE}
levene.test(log(data$TP_bps), data$Condition, location = "median")
```



# Effect of input techniques
__RQ1:__ To which extent does the choice of input techniques influence the pointing performance?

## Categorical coding
```{r include=FALSE}
# copy the Condition column to Condition2
data <- 
  data %>% 
  mutate(Condition2 = Condition)

# change its coding scheme
contrasts(data$Condition2) <- contr.sum(length(levels(data$Condition2)))

```

## Fit the linear model

```{r include=FALSE}
model_org <- lm(TP_bps ~ Condition2, data) # original data
summary(model_org)
model_log <- lm(log(TP_bps + 2) ~ Condition2, data) # original data
summary(model_log)

```

## Test assumption on model residuals (original data)
```{r echo=FALSE}
# Normality of model residuals on original data
data_aug <- data %>% add_residuals(model_org)
qqPlot(data_aug$resid)
tidy(lillie.test(data_aug$resid))
rm(data_aug)

```

```{r echo=FALSE}
# Homogeniety of model residuals on original data
ncvTest(model_org)
```

## Test assumption on model residuals (log data)
```{r echo=FALSE}
# Normality of model residuals on log data
data_aug <- data %>% add_residuals(model_log)
qqPlot(data_aug$resid)
tidy(lillie.test(data_aug$resid))
rm(data_aug)
```

```{r echo=FALSE}
# Homogeniety of model residuals on log data
ncvTest(model_log)
```

## Model with random intercept
```{r include=FALSE}
model_lmer <- lmer(TP_bps ~ (1|Participant) + Condition2, data) 
summary(model_lmer)
```

## Analysis of variance (ANOVA)
```{r include=FALSE}
Anova(model_lmer)

```

# Generalized linear hypothesis
## Pairwise comparison
```{r include=FALSE}
pairwise_org <- glht(model_lmer, linfct = mcp(Condition2 = "Tukey"))
ci_pairwise_org <- tidy(confint(pairwise_org))
pairwise_org <-
  ci_pairwise_org %>%
  ggplot(aes(x = lhs, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, color = "red") +
  xlab("hypothesis") +
  ylab("throughput") +
  coord_flip()
```


# Learning effect
__RQ2:__ To which extent does the learning effect influences the pointing performance?

## Fit the linear model

```{r include=FALSE}
model2_org <- lm(TP_bps ~ Block, data) # original data
summary(model2_org)
model2_log <- lm(log(TP_bps + 2) ~ Block, data) # original data
summary(model2_log)

```

## Test assumption on model residuals (original data)
```{r echo=FALSE}
# Normality of model residuals on original data
data_aug <- data %>% add_residuals(model2_org)
qqPlot(data_aug$resid)
tidy(lillie.test(data_aug$resid))
rm(data_aug)

```

```{r echo=FALSE}
# Homogeniety of model residuals on original data
ncvTest(model2_org)
```

## Test assumption on model residuals (log data)
```{r echo=FALSE}
# Normality of model residuals on log data
data_aug <- data %>% add_residuals(model2_log)
qqPlot(data_aug$resid)
tidy(lillie.test(data_aug$resid))
rm(data_aug)
```

```{r echo=FALSE}
# Homogeniety of model residuals on log data
ncvTest(model2_log)
```

## Model with random intercept
```{r include=FALSE}
model2_lmer <- lmer(TP_bps ~ (1|Participant) + Block, data) 
summary(model2_lmer)
```

## Analysis of variance (ANOVA)
```{r include=FALSE}
Anova(model2_lmer)
```

# Generalized linear hypothesis
## Pairwise comparison
```{r include=FALSE}
pairwise_org <- glht(model2_lmer, linfct = mcp(Block = "Tukey"))
ci_pairwise_org <- tidy(confint(pairwise_org))
pairwise_org <-
  ci_pairwise_org %>%
  ggplot(aes(x = lhs, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, color = "red") +
  xlab("hypothesis") +
  ylab("throughput") +
  coord_flip()
```

# Conclusion
In this section, you will write about what you learned by drawing from both RQ1 and RQ2


(For grading `group_seed`: (write down the number of your `group_seed` here))



# References



